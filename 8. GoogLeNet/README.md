VGGNet验证了加深模型有助于提高网络的性能，GoogLeNet则进一步增加了网络结构的深度，同时使用了Inception模块来增加网络的宽度。

原始的Incpetion模块由1x1,3x3,5x5卷积，3x3最大池化以及一个过滤拼接层组成。一个较大的问题是5x5的卷积分支即使用中等规模的卷积核个数，在计算的代价上也可能难以承受，而且问题会在混合池化之后更为突出，导致输出单元不可避免地增加。

为了克服原始的Inception模块的困难，借鉴NiN（Network in Network）中提出的思想，一种可行的策略是使用1x1卷积进行降维，同时1×1 conv还可以为网络增强非线性。得到降维的Inception模块也称Inception V1模块。

```
横向的卷积核排列设计，使得多个不同size的卷积核能够得到图像当中不同cluster的信息 ，我们称之为“多尺度”。这样融合了不同尺度的卷积以及池化，一个模块一层就可以得到多个尺度的信息，下一阶段也可以同时从不同尺度中提取的特征，可以进行多维度特征的融合，所以效果更好。把计算力拓宽，也避免了太深训练梯度弥散的问题。
对深度相对较大的网络来说，梯度反向传播能够通过所有层的能力就会降低。文中指出：“在这个任务上，更浅网络的强大性能表明网络中部层产生的特征应该是非常有识别力的”。通过将辅助分类器添加到这些中间层，可以提高较低阶段分类器的判别力，这是在提供正则化的同时克服梯度消失问题。后面的实验表明辅助网络的影响相对较小（约0.5），只需要其中一个辅助分类器就能取得同样的效果。
较高层会捕获较高的抽象特征，其空间集中度会减少。这表明随着网络转移到更高层，Inception架构中3×3和5×5卷积的比例应该会增加。而到最后一个卷积层出来的全连接，由全局平均池化替代了，减少了参数，节约了计算量。
```

```
GoogLeNet 最早是發表在 Google 的 Paper：Going deeper with convolutions，裡面介紹了 Inception V1/GoogLenet 架構，並在 ILSVRC-2014 比賽中在分辨項目第一名(Top-5 Error=6.67%)，他只有約 6.8 百萬個參數，比 AlexNet 少九倍，更比 VGG-16 少二十倍，也就是說 GoogleNet 的模型更為輕巧。
```

但在使用原始网络跑CIFAR-100时发现一些问题，无论对前几层采用下列设置时，成功率均不高：
```
3x3, 3x3 -> 100 epoch后 10%卡住 
5x5, 2x2 -> 更低，5%左右
7x7, 3x3 -> 也只有10%左右
```
对网络进行简单修改，去除前四层后增长明显，但是因为没有下采样，速度下降很多，一轮要240s。
```
50000/50000 [] - 240s 5ms/sample - loss: 3.4047 - acc: 0.3731 - val_loss: 4.3349 - val_acc: 0.2295
Epoch 27/100
50000/50000 [] - 241s 5ms/sample - loss: 3.3586 - acc: 0.3864 - val_loss: 4.3287 - val_acc: 0.2279
Epoch 28/100
```
因此准确率下降一定是前四层导致的。逐层恢复，恢复了一层，3x3x192 conv2d+ 3x3 pooling，速度快了一倍，准确率稍有下滑。
```
50000/50000 [] - 93s 2ms/sample - loss: 3.7009 - acc: 0.2990 - val_loss: 3.9187 - val_acc: 0.2590
Epoch 28/100
50000/50000 [] - 95s 2ms/sample - loss: 3.6734 - acc: 0.3049 - val_loss: 4.1051 - val_acc: 0.2198
Epoch 29/100
```

再恢复一层1x1x64 conv2d，时间到了105s左右一轮，整体上准确率几乎无变化。
```
Epoch 18/100
50000/50000 [] - 102s 2ms/sample - loss: 4.3021 - acc: 0.2421 - val_loss: 4.4836 - val_acc: 0.1988
Epoch 19/100
50000/50000 [] - 104s 2ms/sample - loss: 4.1884 - acc: 0.2477 - val_loss: 5.6923 - val_acc: 0.0873
...
Epoch 28/100
50000/50000 [] - 105s 2ms/sample - loss: 3.7984 - acc: 0.2844 - val_loss: 4.4914 - val_acc: 0.1696
Epoch 29/100
50000/50000 [] - 104s 2ms/sample - loss: 3.7771 - acc: 0.2913 - val_loss: 4.2550 - val_acc: 0.2093
```

恢复一层2x2的池化层，速度如预期中一样提高了40%，63s。但18轮开始便卡在10%附近：
```
Epoch 18/100
50000/50000 [] - 61s 1ms/sample - loss: 4.6721 - acc: 0.1267 - val_loss: 4.7533 - val_acc: 0.1053
Epoch 19/100
50000/50000 [] - 59s 1ms/sample - loss: 4.5735 - acc: 0.1271 - val_loss: 4.6366 - val_acc: 0.1091
```
那这个问题则很有可能是因为上一层卷积层导致的，恢复第一层7x7x64卷积层，训练时间大概在50s左右一轮。准确率下降到10%附近，且loss卡在4.2左右。
```
Epoch 28/100
50000/50000 [] - 49s 971us/sample - loss: 4.2579 - acc: 0.1186 - val_loss: 4.2499 - val_acc: 0.1203
Epoch 29/100
50000/50000 [] - 48s 967us/sample - loss: 4.2519 - acc: 0.1171 - val_loss: 4.3188 - val_acc: 0.1021
```
问题分析
=============== 
根据上面的观测数据，准确率不高极有可能是这个2x2的池化层导致的，原因是什么？

看下没有2x2池化时的结构：
```
_________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 64)   256         input_1[0][0]                    
__________________________________________________________________________________________________
...


conv2d_55 (Conv2D)              (None, 4, 4, 128)    106624      max_pooling2d_11[0][0]           
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 4, 4, 1024)   0           conv2d_50[0][0]                  
                                                                 conv2d_52[0][0]                  
                                                                 conv2d_54[0][0]                  
                                                                 conv2d_55[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 2, 2, 1024)   0           concatenate_8[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4096)         0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          409700      flatten[0][0]                    
==================================================================================================
Total params: 6,369,876
Trainable params: 6,369,876
Non-trainable params: 0
```
而带上池化层之后：

```
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 16, 16, 3)    0           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 16, 16, 64)   256         max_pooling2d[0][0]              

...
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 2, 2, 128)    106624      max_pooling2d_12[0][0]           
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 2, 2, 1024)   0           conv2d_50[0][0]                  
                                                                 conv2d_52[0][0]                  
                                                                 conv2d_54[0][0]                  
                                                                 conv2d_55[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 1024)   0           concatenate_8[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 1024)         0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          102500      flatten[0][0]                    
==================================================================================================
Total params: 6,062,676
Trainable params: 6,062,676
Non-trainable params: 0
__________________________________________________________________________________________________

```

恢复原来第一个卷积层，得到形状：
``` 
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 16, 16, 64)   9472        input_1[0][0]                    
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 8, 8, 64)     0           conv2d[0][0]                     

...
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 2, 2, 832)    0           conv2d_39[0][0]                  
                                                                 conv2d_41[0][0]                  
                                                                 conv2d_43[0][0]                  
                                                                 conv2d_44[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D) (None, 1, 1, 832)    0           concatenate_6[0][0]              
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 1, 1, 160)    133280      max_pooling2d_10[0][0]           
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 1, 1, 32)     26656       max_pooling2d_10[0][0]           
__________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D) (None, 1, 1, 832)    0           max_pooling2d_10[0][0]           

...

__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 1, 1, 128)    106624      max_pooling2d_12[0][0]           
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 1, 1, 1024)   0           conv2d_51[0][0]                  
                                                                 conv2d_53[0][0]                  
                                                                 conv2d_55[0][0]                  
                                                                 conv2d_56[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 1024)   0           concatenate_8[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 1024)         0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          102500      flatten[0][0]                    
==================================================================================================
Total params: 6,076,052
Trainable params: 6,076,052
Non-trainable params: 0
__________________________________________________________________________________________________


```

可以看到，因为CIFAR-100原始输入只有32x32x3，经过这么多层处理之后，图片已经几乎没了，最底下只剩1x1，这实在是太少了，中间很多层实际上都没有发挥作用。

去掉前三层后，最终有2x2x32，看起来还稍微正常一点，而且这么一折腾，准确率居然再次提高。
``` 
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 192)  5376        input_1[0][0]                    
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 16, 16, 192)  0           conv2d[0][0]                     
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 16, 16, 96)   18528       max_pooling2d[0][0]              
...

__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 4, 4, 1024)   0           conv2d_49[0][0]                  
                                                                 conv2d_51[0][0]                  
                                                                 conv2d_53[0][0]                  
                                                                 conv2d_54[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 2, 2, 1024)   0           concatenate_8[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4096)         0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          409700      flatten[0][0]                    
==================================================================================================
Total params: 6,264,212
Trainable params: 6,264,212
Non-trainable params: 0
__________________________________________________________________________________________________
```

尝试删除inception。删除了5组inception之后，准确率似乎下降了，训练时间也变长了很多（因为图下采样次数变少了），删除4组，3组也类似：
```

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 192)  5376        input_1[0][0]                    
__________________________________________________________________________________________________

...

conv2d_35 (Conv2D)              (None, 8, 8, 128)    153728      conv2d_34[0][0]                  
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 8, 8, 128)    106624      max_pooling2d_7[0][0]            
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 8, 8, 1024)   0           conv2d_31[0][0]                  
                                                                 conv2d_33[0][0]                  
                                                                 conv2d_35[0][0]                  
                                                                 conv2d_36[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 4, 4, 1024)   0           concatenate_5[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 16384)        0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          1638500     flatten[0][0]                    
==================================================================================================
Total params: 6,449,204
Trainable params: 6,449,204
Non-trainable params: 0
```

同等情况下，删除1～2组inception时效果最好，删除3组之后，虽然准确率有轻微提升，但训练速度却下降很多（因下采样的少，图片变大了），所以最好的还是删除1组inception比较适合CIFAR-100的样本集。这时训练速度为85s一轮，但是准确率和240s一轮的不下采样几乎一样。
```
Epoch 27/100
50000/50000 [] - 84s 2ms/sample - loss: 3.6861 - acc: 0.3116 - val_loss: 4.7174 - val_acc: 0.1643
Epoch 28/100
50000/50000 [] - 84s 2ms/sample - loss: 3.6697 - acc: 0.3141 - val_loss: 4.0279 - val_acc: 0.2560
……
Epoch 99/100
50000/50000 [] - 83s 2ms/sample - loss: 3.4028 - acc: 0.4958 - val_loss: 4.8031 - val_acc: 0.2777
Epoch 100/100
50000/50000 [] - 83s 2ms/sample - loss: 3.3852 - acc: 0.4988 - val_loss: 4.7725 - val_acc: 0.2876

Process finished with exit code 0
```

但整体效果还是不好。而且还过拟合了，把dropout加回去。虽然过拟合问题得到解决，但loss却仍然一直下不去

```
Epoch 28/100
50000/50000 [] - 84s 2ms/sample - loss: 3.8378 - acc: 0.2427 - val_loss: 4.0739 - val_acc: 0.2090
Epoch 29/100
50000/50000 [] - 83s 2ms/sample - loss: 3.8277 - acc: 0.2453 - val_loss: 4.0058 - val_acc: 0.2061
Epoch 30/100
50000/50000 [] - 85s 2ms/sample - loss: 3.8182 - acc: 0.2472 - val_loss: 4.0294 - val_acc: 0.2065
Epoch 31/100
27584/50000 [===============>..............] - ETA: 36s - loss: 3.7966 - acc: 0.2546
```

加上数据增强以后只是改变了过拟合的情况，成功率上并没有什么特别的好转：
```
Epoch 26/100
782/782 [] - 87s 111ms/step  - loss: 3.9232 - acc: 0.2107 - val_loss: 3.8438 - val_acc: 0.2291
Epoch 27/100
782/782 [] - 87s 111ms/step  - loss: 3.9105 - acc: 0.2105 - val_loss: 3.8559 - val_acc: 0.2292
Epoch 28/100
```

```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 64)   256         input_1[0][0]                    

...
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 4, 4, 1024)   0           conv2d_44[0][0]                  
                                                                 conv2d_46[0][0]                  
                                                                 conv2d_48[0][0]                  
                                                                 conv2d_49[0][0]                  
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 2, 2, 1024)   0           concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2, 2, 1024)   0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
flatten (Flatten)               (None, 4096)         0           dropout[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 100)          409700      flatten[0][0]                    
==================================================================================================
Total params: 6,183,652
Trainable params: 6,183,652
Non-trainable params: 0
__________________________________________________________________________________________________
```

经过上述操作之后仍然只有26的成功率，实在是太低了，根据网上看的，应该有65%左右的成功率才对。
```
Epoch 69/100
782/782 [] - 93s 120ms/step  - loss: 3.7855 - acc: 0.2644 - val_loss: 3.6334 - val_acc: 0.2976
Epoch 70/100
782/782 [] - 98s 126ms/step  - loss: 3.7829 - acc: 0.2637 - val_loss: 3.8383 - val_acc: 0.2589
Epoch 71/100
782/782 [] - 102s 130ms/step  - loss: 3.7870 - acc: 0.2629 - val_loss: 3.6722 - val_acc: 0.2895
Epoch 72/100
782/782 [] - 97s 124ms/step  - loss: 3.7706 - acc: 0.2667 - val_loss: 3.7440 - val_acc: 0.2906
Epoch 73/100
782/782 [] - 101s 129ms/step  - loss: 3.7712 - acc: 0.2667 - val_loss: 3.6629 - val_acc: 0.2912
Epoch 74/100
782/782 [] - 98s 125ms/step  - loss: 3.7756 - acc: 0.2673 - val_loss: 3.7076 - val_acc: 0.2884
Epoch 75/100
```

回想到之前的结论，那是不是可以从strides入手呢？因为GoogLeNet原来是为了处理分辨率为224x224的图片的，前面的步骤也有一个隐含的下采样的作用。照理来说，经过它们下采样几轮以后，图片大小将很像CIFAR-100的输入，即32x32。那如果我们这里不再strides=2，而是strides=1，保留网络层级（但网络中的一些卷积核和strides有调整）之后，准确率最高可以达到55%。
```
可以看到，因为输入只有32x32x3，经过这么多层处理之后，图片已经几乎没了，最底下只剩1x1，这实在是太少了，中间很多层实际上都没有发挥作用。

去掉前三层后，最终有2x2x32，看起来还稍微正常一点，而且这么一折腾，准确率居然再次提高。
```

另外，回想一下，它提出时是为了参加比赛，比赛给的数据则是224x224x3的，如果我把32x32x3的CIFAR-100拉伸到224x224x3，并用原始网络会怎样？复制了一下网上的拉伸代码，即使用CV2来拉伸：

```
_x_train = []
_x_test = []
for x_tr in x_train:
    _x_train.append(cv2.resize(x_tr, (224, 224)))
for x_te in x_test:
    _x_test.append(cv2.resize(x_te, (224, 224)))
print(np.array(_x_train).shape)
_x_train = np.array(_x_train).reshape(-1, 224, 224, 3)
_x_test = np.array(_x_test).reshape(-1, 224, 224, 3)
```

再运用初始网络配置，可以明显发现准确率提升是很快的，至少没有出现我直接丢32x32x3时那样卡在20%左右的情况，不过最终也是停在50%附近。而且有一个明显问题，就是因为处理的图片变大了很多倍，网络的处理速度也慢了很多，一轮下来居然要接近330s。

考虑到网络对准确度没有产生明显提升，但速度却慢了300%，因此我认为还是使用原来那种网络比较好。这可能是因为我们去除了前几层的下采样，导致输入的32x32x3实际上是比它下采样完更“优质”的图片（毕竟没有对输入做任何改动），而且因为32x32的图片也很小，网络处理起来速度也非常快。

对于不同的数据集，不是网络越深越好，如果网络过深，可以尝试去除一部分网络再调试，效果可能会更好。