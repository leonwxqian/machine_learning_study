这次实现VGG19。首先依然是用keras编写。与VGG类似的，它们只相差3个隐藏层，分别位于最后三个池化层之前，因此直接修改VGG16的代码将很容易实现VGG19。VGG19与VGG16基本类似，只是二者相差3个卷积层。VGG16包含13个卷积层和3个全连接层，而VGG19则由16个卷积层和3个全连接层组成。


相较于CNN，VGG的训练耗费明显参数量大幅增加，而且消耗了更多的内存。这里绝大多数的参数都是来自于第一个全连接层。

一些参考资料：`https://bbs.huaweicloud.com/forum/forum.php?mod=viewthread&tid=85088`

使用keras实现VGG19
===================
对着文章中那个表格Class E即VGG19，从Class D(VGG16)简单修改后进行训练得到： 
```
50000/50000 [==============================] - 100s 2ms/sample - loss: 8.2701 - acc: 0.0156 - val_loss: 7.3657 - val_acc: 0.0257
Epoch 2/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 7.6863 - acc: 0.0287 - val_loss: 7.2070 - val_acc: 0.0300
Epoch 3/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 7.3941 - acc: 0.0400 - val_loss: 7.0985 - val_acc: 0.0349
Epoch 4/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 7.2125 - acc: 0.0475 - val_loss: 7.1261 - val_acc: 0.0281
Epoch 5/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 7.0228 - acc: 0.0549 - val_loss: 7.2031 - val_acc: 0.0559
Epoch 6/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 6.8878 - acc: 0.0650 - val_loss: 7.0377 - val_acc: 0.0586
Epoch 7/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.7594 - acc: 0.0725 - val_loss: 9.9782 - val_acc: 0.0635
Epoch 8/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.6355 - acc: 0.0800 - val_loss: 8.9516 - val_acc: 0.0714
Epoch 9/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.5210 - acc: 0.0885 - val_loss: 8.6280 - val_acc: 0.0741
Epoch 10/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.4191 - acc: 0.0967 - val_loss: 7.4332 - val_acc: 0.0785
Epoch 11/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.3247 - acc: 0.1013 - val_loss: 8.3964 - val_acc: 0.0792
Epoch 12/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.2239 - acc: 0.1078 - val_loss: 6.6852 - val_acc: 0.0889
Epoch 13/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.1059 - acc: 0.1180 - val_loss: 7.3093 - val_acc: 0.0962
Epoch 14/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.0065 - acc: 0.1259 - val_loss: 7.5560 - val_acc: 0.1082
Epoch 15/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 5.9108 - acc: 0.1355 - val_loss: 9.6596 - val_acc: 0.0988
Epoch 16/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 5.8050 - acc: 0.1470 - val_loss: 7.7643 - val_acc: 0.1229
Epoch 17/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 5.6976 - acc: 0.1589 - val_loss: 6.9461 - val_acc: 0.1514
Epoch 18/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 5.6017 - acc: 0.1713 - val_loss: 7.5841 - val_acc: 0.1389
Epoch 19/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 5.4963 - acc: 0.1837 - val_loss: 8.1439 - val_acc: 0.1782
Epoch 20/100
50000/50000 [==============================] - 92s 2ms/sample - loss: 5.4041 - acc: 0.1913 - val_loss: 6.4107 - val_acc: 0.1937
Epoch 21/100
50000/50000 [==============================] - 93s 2ms/sample - loss: 5.3080 - acc: 0.2035 - val_loss: 5.9450 - val_acc: 0.1629
Epoch 22/100
50000/50000 [==============================] - 92s 2ms/sample - loss: 5.2038 - acc: 0.2163 - val_loss: 5.8420 - val_acc: 0.1852
Epoch 23/100
50000/50000 [==============================] - 92s 2ms/sample - loss: 5.1028 - acc: 0.2304 - val_loss: 5.8413 - val_acc: 0.2060
Epoch 24/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 5.0237 - acc: 0.2391 - val_loss: 6.4759 - val_acc: 0.2107
Epoch 25/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 4.9100 - acc: 0.2520 - val_loss: 5.6759 - val_acc: 0.2221
Epoch 26/100
50000/50000 [==============================] - 91s 2ms/sample - loss: 4.8271 - acc: 0.2647 - val_loss: 5.5233 - val_acc: 0.2660
Epoch 27/100
50000/50000 [==============================] - 92s 2ms/sample - loss: 4.7329 - acc: 0.2761 - val_loss: 5.9107 - val_acc: 0.2474
Epoch 28/100
50000/50000 [==============================] - 94s 2ms/sample - loss: 4.6558 - acc: 0.2854 - val_loss: 6.5451 - val_acc: 0.2095
Epoch 29/100
50000/50000 [==============================] - 101s 2ms/sample - loss: 4.5580 - acc: 0.3043 - val_loss: 5.2173 - val_acc: 0.2771
Epoch 30/100
50000/50000 [==============================] - 101s 2ms/sample - loss: 4.4970 - acc: 0.3127 - val_loss: 5.2754 - val_acc: 0.3077
```

因训练时间较长，这里我训练了30轮便停止了。对比VGG16的实现，确认一下二者的差异：
```
VGG19：
Epoch 10/100
50000/50000 [==============================] - 90s 2ms/sample - loss: 6.4191 - acc: 0.0967 - val_loss: 7.4332 - val_acc: 0.0785

VGG16：
Epoch 10/100
50000/50000 [==============================] - 74s 1ms/sample - loss: 5.6551 - acc: 0.1266 - val_loss: 5.9755 - val_acc: 0.1190

VGG19:
Epoch 20/100
50000/50000 [==============================] - 92s 2ms/sample - loss: 5.4041 - acc: 0.1913 - val_loss: 6.4107 - val_acc: 0.1937

VGG16：
Epoch 20/100
50000/50000 [==============================] - 74s 1ms/sample - loss: 4.4908 - acc: 0.2768 - val_loss: 7.1203 - val_acc: 0.1827

VGG19:
Epoch 30/100
50000/50000 [==============================] - 101s 2ms/sample - loss: 4.4970 - acc: 0.3127 - val_loss: 5.2754 - val_acc: 0.3077

VGG16:
Epoch 30/100
50000/50000 [==============================] - 75s 1ms/sample - loss: 3.6325 - acc: 0.4249 - val_loss: 3.8966 - val_acc: 0.4186
```

可以看到，VGG19显著的延长了训练时间，每一轮的训练时间多了20秒左右。但因为我只训练了一次，30轮，而这三十轮里二者准确率互相交叉，但又都没收敛，因此没有什么定论。不过至少，它俩准确率差距显然不是那种天差地别的。


使用tensorflow底层API实现
==============

为了方便调用，我将底层api封装成包 nnlayer。

因此这里的代码我将直接import nnlayer，然后基于它去实现。实现后准确率移植维持在一个很低的水平，这明显是不对的，下一次我将对比另一个项目https://github.com/machrisaa/tensorflow-vgg，尝试定位具体原因，确定为什么我自己实现的相差如此之大。

```
step 781, acc: 0.013
step 1562, acc: 0.016
step 2343, acc: 0.016
step 3124, acc: 0.019
step 3905, acc: 0.021
step 4686, acc: 0.016
step 5467, acc: 0.017
step 6248, acc: 0.029
step 7029, acc: 0.028
step 7810, acc: 0.023
step 8591, acc: 0.027
step 9372, acc: 0.023 
```
